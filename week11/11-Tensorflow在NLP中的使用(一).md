

## TensorFlow在NLP的使用（一）

### 一、Word2vec

word2vec 是 Google 于 2013 年开源的一个用于获取词向量的工具包，作者是 Tomas Mikolov。

它是一个将词表示为一个向量的工具，通过该向量表示，可以用来进行更深入的自然语言处理，比如机器翻译等。

网上资料：

- [自己动手写word2vec 系列文章](https://blog.csdn.net/u014595019/article/details/51884529)
- [word2vec 笔记](https://blog.csdn.net/zhangxb35/article/details/74716245)
- ......

当我们分析图片或者语音的时候，我们通常都是在分析密集的，高纬度的数据集。我们所需的全部信息都储存在原始数据中。

![](http://p35l3ejfq.bkt.clouddn.com/20181012143644.png)

当我们处理自然语言问题的时候，我们通常会做分词，然后给每一个词一个编号，比如猫的编号是 120，狗的编号是 343。比如女生的编号是 1232，女王的编号是 2329。这些编号是没有规律，没有联系的，我们从编号中不能得到词与词之间的相关性。

例如：How are you？

How : 234
Are : 7
you : 987

如果把上面转换为 one-hot 方式：

``` xml
000…1000000…
00000001000…
000…0000010
```

但是你从编号中看不到词与此之间什么相关性。

### 二、Word2vec两种模型

1）连续词袋模型（CBOW）

根据词的上下文词汇来预测目标词汇，例如上下文词汇是“今天早餐吃__”，要预测的目标词汇可能是“面包”。

2）Skip-Gram模型

Skip-Gram模型刚好和CBOW相反，它是通过目标词汇来预测上下文词汇。例如目标词汇是“早餐”，上下文词汇可能是“今天”和“吃面包”。

![](http://p35l3ejfq.bkt.clouddn.com/20181012144601.png)

对于这两种模型的训练，我们可能容易想到，使用 softmax 作为输出层来训练网络。这个方法是可
行的，只不过使用 softmax 作为输出层计算量将会是巨大的。假如我们已知上下文，需要训练模型
预测目标词汇，假设总共有 50000 个词汇，那么每一次训练都需要计算输出层的 50000 个概率值。

所以训练 Word2vec 模型我们通常可以选择使用噪声对比估计（Noise Contrastive Estimation）
。NCE 使用的方法是把上下文 h 对应地正确的目标词汇标记为正样本（D=1），然后再抽取一些错
误的词汇作为负样本（D=0）。然后最大化目标函数的值。

![](http://p35l3ejfq.bkt.clouddn.com/20181012144725.png)

当真实的目标单词被分配到较高的概率，同时噪声单词的概率很低时，目标函数也就达到最大值
了。计算这个函数时，只需要计算挑选出来的k个噪声单词，而不是整个语料库。所以训练速度会
很快。

### 三、Word2vec图形化

![](http://p35l3ejfq.bkt.clouddn.com/20181012144910.png)

### 四、CNN在自然语言处理的应用

说到 CNN 我们首先可能会想到 CNN 在计算机视觉中的应用。近几年 CNN 也开始应用于自然语言处
理，并取得了一些引人注目的成绩。

CNN 应用于 NLP 的任务，处理的往往是以矩阵形式表达的句子或文本。矩阵中的每一行对应于一
个分词元素，一般是一个单词，也可以是一个字符。也就是说每一行都是一个词或者字符的向量
（比如前面说到的 word2vec）。假设我们一共有 10 个词，每个词都用 128 维的向量来表示，那么
我们就可以得到一个 10×128 维的矩阵。这个矩阵就相当于是一副“图像”。

![](http://p35l3ejfq.bkt.clouddn.com/20181012145126.png)

GitHub 上的例子：[cnn-text-classification-tf](https://github.com/dennybritz/cnn-text-classification-tf)



